
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Taha Bouhsine</title>
    <meta name="description" content="Jack of all trades, Master of Data Science."> <link rel="stylesheet" href="/css/index.css">
    <link
    rel="stylesheet" href="/css/bootstrap.min.css"> 
    <link rel="stylesheet" href="/css/animate.min.css">
    <link rel="stylesheet" href="/css/aos.css">
    <link rel="stylesheet" href="/css/style.css">
    
      
        <link rel="stylesheet" href="/css/ionicons/css/ionicons.min.css"/>
      
    
    <link rel="stylesheet" href="/css/prism-base16-monokai.dark.css">
    <link rel="stylesheet" href="/css/prism-diff.css">
    <link rel="alternate" href="/feed/feed.xml" type="application/atom+xml" title="Taha Bouhsine">
    <link rel="alternate" href="/feed/feed.json" type="application/json" title="Taha Bouhsine"> 
    <!-- jQuery --><script src="/js/jquery.js"> </script>
    <!--  plugins  -->
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/plugins.js"></script>
    <script src="/js/aos.js"></script>
    <script src="/js/jquery.form.js"></script>
    <script src="/js/jquery.validate.min.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-W820CG4X4V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag('js', new Date());
      gtag('config', 'G-W820CG4X4V');
    </script>
    <!--  main script  -->
    <script src="/js/custom.js"></script>
  </head>
  <body>
    <div id="preloader">
      <div id="status">
        <div class="preloader" aria-busy="true" aria-label="Loading, please wait." role="progressbar"></div>
      </div>
    </div>
    
<header class="navbar-fixed-top">
    <nav>
        <ul>
                <li class="nav-item">
                    <a href="/">Home</a>
                </li>
                <li class="nav-item">
                    <a href="/posts/">Archive</a>
                </li>
                <li class="nav-item">
                    <a href="/about/">About Me</a>
                </li>
        </ul>
    </nav>
</header>
<div class="animatedModal-on">
    <div class="clearfix"></div>
    <div class="modal-content">
        <div class="container">
            <div class="portfolio-padding">
                <div class="col-md-8 col-md-offset-2">
                    <h2>Seeing the Unseen - SeeNN Multimodal ML System for Atmospheric Visibility Estimation</h2>
                    <div class="h-50"></div>

                    
                    <p><strong>Overview:</strong></p><p>Our ongoing project focuses on developing an advanced multimodal machine learning system tailored for in-flight long-range atmospheric visibility estimation. The system leverages RGB (color) and Depth data, collected from various sources like cameras and depth sensors, to accurately predict visibility conditions during flight.</p><p>Our multimodal machine learning system, designed and trained using TensorFlow and Keras, serves as a valuable tool for in-flight long-range atmospheric visibility estimation. By combining RGB and Depth data in a unified model, we aim to provide accurate and reliable visibility predictions during flight operations. The application of this technology extends to various domains, including aviation, environmental monitoring, and weather prediction, ensuring safer and more efficient air travel experiences.</p><br><h3>System Overview:</h3><p>Our first step involves data preprocessing, where we clean, handle missing values, and align the RGB and Depth data to ensure smooth fusion during the training phase. TensorFlow and Keras are our primary tools for building the machine learning model.</p><h3>Model Architecture:</h3>The core of our system is a sophisticated multimodal deep learning architecture that seamlessly integrates the RGB and Depth information. The RGB model consists of Convolutional Layers, Pooling Layers, and a Flatten layer, while the Depth model mirrors the RGB model's structure, ending in a cross-attention multimodal fusion technique. These models are trained using TensorFlow and Keras, enabling us to optimize their performance efficiently.<h4>Mermaid Diagram: Model Architecture</h4><center><img src='https://i.imgur.com/DKoozw9.png' style='width: 450px; max-width: 950px;'></center><h3>Application Flow</h3><p>Once trained, our system is ready for real-world applications. During in-flight operations, pilots and relevant personnel can interact with the system through a user-friendly interface. By providing images or video streams containing RGB and Depth data, they initiate the visibility estimation process. The input data undergoes preprocessing and is then fed into the trained multimodal model for accurate visibility prediction.</p></div><br>
                    

                </div>
            </div>
        </div>
    </div>
</div>
  </body>
</html>